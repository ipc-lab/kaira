"""
====================================================================
Advanced LDPC Code Visualization with Belief Propagation Animation
====================================================================

This example demonstrates advanced visualizations for Low-Density Parity-Check
(LDPC) codes, including animated belief propagation, Tanner graph analysis,
and performance comparisons with different decoder configurations.
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import torch
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.patches import Circle, FancyArrowPatch, Rectangle
from tqdm import tqdm

from kaira.channels.analog import AWGNChannel
from kaira.models.fec.decoders import BeliefPropagationDecoder
from kaira.models.fec.encoders import LDPCCodeEncoder

# %%
# Setting up
# --------------------------------------
torch.manual_seed(42)
np.random.seed(42)

# Configure visualization settings
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_context("notebook", font_scale=1.2)

# Custom color schemes
belief_cmap = LinearSegmentedColormap.from_list("belief", ["#d32f2f", "#ffeb3b", "#4caf50"], N=256)
modern_palette = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd"]

# %%
# Enhanced Tanner Graph Visualization
# --------------------------------------
# Create a more sophisticated Tanner graph with better positioning


def create_enhanced_tanner_graph(H, title="LDPC Tanner Graph"):
    """Create an enhanced Tanner graph visualization with optimized layout."""
    m, n = H.shape  # m check nodes, n variable nodes

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9), constrained_layout=True)

    # Left plot: Traditional Tanner graph
    ax1.set_title(f"{title} - Bipartite Graph", fontsize=14, fontweight="bold")

    # Position variable nodes in a circle (top)
    var_angles = np.linspace(0, 2 * np.pi, n, endpoint=False)
    var_positions = [(2 * np.cos(angle), 2 * np.sin(angle) + 1) for angle in var_angles]

    # Position check nodes in a circle (bottom)
    check_angles = np.linspace(0, 2 * np.pi, m, endpoint=False)
    check_positions = [(1.5 * np.cos(angle), 1.5 * np.sin(angle) - 1) for angle in check_angles]

    # Draw connections with different styles based on degree
    connection_counts = np.sum(H, axis=0)  # variable node degrees
    max_degree = np.max(connection_counts)

    for i in range(m):
        for j in range(n):
            if H[i, j] == 1:
                # Line thickness based on variable node degree
                thickness = 1 + 2 * (connection_counts[j] / max_degree)
                alpha = 0.6 + 0.4 * (connection_counts[j] / max_degree)

                line = FancyArrowPatch(check_positions[i], var_positions[j], arrowstyle="-", color="gray", linewidth=thickness, alpha=alpha, connectionstyle="arc3,rad=0.1")
                ax1.add_patch(line)

    # Draw variable nodes with size based on degree
    for j, pos in enumerate(var_positions):
        size = 0.15 + 0.15 * (connection_counts[j] / max_degree)
        circle = Circle(pos, size, facecolor=modern_palette[0], edgecolor="black", linewidth=2, zorder=10)
        ax1.add_patch(circle)
        ax1.text(pos[0], pos[1], f"v{j}", ha="center", va="center", fontsize=10, fontweight="bold", color="white", zorder=11)
        # Show degree
        ax1.text(pos[0], pos[1] - size - 0.3, f"d={int(connection_counts[j])}", ha="center", va="center", fontsize=8, zorder=11)

    # Draw check nodes
    check_degrees = np.sum(H, axis=1)
    max_check_degree = np.max(check_degrees)

    for i, pos in enumerate(check_positions):
        size = 0.15 + 0.15 * (check_degrees[i] / max_check_degree)
        square = Rectangle((pos[0] - size, pos[1] - size), 2 * size, 2 * size, facecolor=modern_palette[3], edgecolor="black", linewidth=2, zorder=10)
        ax1.add_patch(square)
        ax1.text(pos[0], pos[1], f"c{i}", ha="center", va="center", fontsize=10, fontweight="bold", color="white", zorder=11)
        # Show degree
        ax1.text(pos[0], pos[1] + size + 0.3, f"d={int(check_degrees[i])}", ha="center", va="center", fontsize=8, zorder=11)

    # Set axis properties
    ax1.set_xlim(-3.5, 3.5)
    ax1.set_ylim(-3.5, 3.5)
    ax1.set_aspect("equal")
    ax1.axis("off")

    # Add legend
    legend_elements = [
        plt.Line2D([0], [0], marker="o", color="w", markerfacecolor=modern_palette[0], markersize=10, label="Variable Nodes"),
        plt.Line2D([0], [0], marker="s", color="w", markerfacecolor=modern_palette[3], markersize=10, label="Check Nodes"),
        plt.Line2D([0], [0], color="gray", linewidth=2, label="Connections"),
    ]
    ax1.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(1, 1))

    # Right plot: Parity check matrix heatmap
    ax2.set_title("Parity Check Matrix H", fontsize=14, fontweight="bold")

    # Create a custom colormap for the matrix
    matrix_cmap = LinearSegmentedColormap.from_list("matrix", ["white", "#2c3e50"])

    im = ax2.imshow(H, cmap=matrix_cmap, interpolation="nearest", aspect="auto")

    # Add text annotations
    for i in range(m):
        for j in range(n):
            ax2.text(j, i, int(H[i, j]), ha="center", va="center", color="black" if H[i, j] == 0 else "white", fontsize=12, fontweight="bold")

    # Customize matrix plot
    ax2.set_xticks(range(n))
    ax2.set_yticks(range(m))
    ax2.set_xlabel("Variable Nodes", fontsize=12)
    ax2.set_ylabel("Check Nodes", fontsize=12)

    # Add colorbar
    cbar = plt.colorbar(im, ax=ax2, shrink=0.8)
    cbar.set_ticks([0, 1])
    cbar.set_ticklabels(["0", "1"])

    # Add sparsity information
    sparsity = np.sum(H) / (m * n)
    ax2.text(0.02, 0.98, f"Sparsity: {sparsity:.3f}\nDensity: {1-sparsity:.3f}", transform=ax2.transAxes, fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8), verticalalignment="top")

    return fig


# %%
# Create LDPC Code and Visualize Tanner Graph
# --------------------------------------------

# Define a more interesting LDPC code
H_matrix = torch.tensor([[1, 1, 0, 1, 0, 0, 0, 1], [0, 1, 1, 0, 1, 1, 0, 0], [1, 0, 1, 0, 0, 1, 1, 0], [0, 0, 0, 1, 1, 0, 1, 1]], dtype=torch.float32)

print("Enhanced LDPC Code Analysis")
print("=" * 40)
print(f"H matrix dimensions: {H_matrix.shape}")
print(f"Expected code dimensions: ({H_matrix.shape[1]}, {H_matrix.shape[1] - H_matrix.shape[0]})")
print(f"Variable node degrees: {torch.sum(H_matrix, dim=0).tolist()}")
print(f"Check node degrees: {torch.sum(H_matrix, dim=1).tolist()}")

# Create LDPC encoder to get actual dimensions
encoder = LDPCCodeEncoder(H_matrix)
print(f"Actual code dimensions: ({encoder.code_length}, {encoder.code_dimension})")
print(f"Actual code rate: {encoder.code_dimension/encoder.code_length:.3f}")

# Create enhanced Tanner graph
create_enhanced_tanner_graph(H_matrix.numpy(), f"Enhanced LDPC Code ({encoder.code_length},{encoder.code_dimension})")
plt.show()

# %%
# Belief Propagation Animation
# --------------------------------------


class BeliefPropagationVisualizer:
    """Visualize belief propagation iterations in LDPC decoding."""

    def __init__(self, H, received_llrs, max_iterations=5):
        self.H = H
        self.m, self.n = H.shape
        self.received_llrs = received_llrs
        self.max_iterations = max_iterations

        # Initialize messages
        self.var_to_check = np.zeros((self.n, self.m))
        self.check_to_var = np.zeros((self.m, self.n))
        self.beliefs = received_llrs.copy()

        # Store history for animation
        self.belief_history = [self.beliefs.copy()]
        self.var_to_check_history = [self.var_to_check.copy()]
        self.check_to_var_history = [self.check_to_var.copy()]

    def step(self):
        """Perform one iteration of belief propagation."""
        # Check node update
        for i in range(self.m):
            for j in range(self.n):
                if self.H[i, j] == 1:
                    # Collect messages from other variable nodes
                    other_vars = [k for k in range(self.n) if k != j and self.H[i, k] == 1]
                    if other_vars:
                        product = 1.0
                        for k in other_vars:
                            product *= np.tanh(self.var_to_check[k, i] / 2)
                        self.check_to_var[i, j] = 2 * np.arctanh(np.clip(product, -0.999, 0.999))
                    else:
                        self.check_to_var[i, j] = 0

        # Variable node update
        for j in range(self.n):
            for i in range(self.m):
                if self.H[i, j] == 1:
                    # Sum messages from other check nodes
                    other_checks = [k for k in range(self.m) if k != i and self.H[k, j] == 1]
                    message_sum = self.received_llrs[j]
                    for k in other_checks:
                        message_sum += self.check_to_var[k, j]
                    self.var_to_check[j, i] = message_sum

            # Update belief
            self.beliefs[j] = self.received_llrs[j] + np.sum(self.check_to_var[:, j])

        # Store history
        self.belief_history.append(self.beliefs.copy())
        self.var_to_check_history.append(self.var_to_check.copy())
        self.check_to_var_history.append(self.check_to_var.copy())

    def run_iterations(self):
        """Run all iterations."""
        for _ in range(self.max_iterations):
            self.step()

    def visualize_iteration(self, iteration):
        """Visualize a specific iteration."""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14), constrained_layout=True)
        fig.suptitle(f"Belief Propagation - Iteration {iteration}", fontsize=16, fontweight="bold")

        # Current beliefs
        beliefs = self.belief_history[iteration]
        hard_decisions = (beliefs > 0).astype(int)

        ax1.bar(range(self.n), beliefs, color=[belief_cmap(0.5 + 0.5 * np.tanh(b / 4)) for b in beliefs], edgecolor="black", linewidth=1)
        ax1.axhline(y=0, color="red", linestyle="--", alpha=0.7)
        ax1.set_title("Variable Node Beliefs (LLRs)")
        ax1.set_xlabel("Variable Node")
        ax1.set_ylabel("Log-Likelihood Ratio")

        # Add hard decision annotations
        for i, (belief, decision) in enumerate(zip(beliefs, hard_decisions)):
            ax1.text(i, belief + 0.2 * np.sign(belief), str(decision), ha="center", va="bottom" if belief > 0 else "top", fontweight="bold", fontsize=12)

        # Variable to check messages
        var_to_check = self.var_to_check_history[iteration]
        im2 = ax2.imshow(var_to_check.T, cmap="RdBu_r", aspect="auto", vmin=-5, vmax=5)
        ax2.set_title("Variable â†’ Check Messages")
        ax2.set_xlabel("Variable Node")
        ax2.set_ylabel("Check Node")
        plt.colorbar(im2, ax=ax2, shrink=0.8)

        # Check to variable messages
        check_to_var = self.check_to_var_history[iteration]
        im3 = ax3.imshow(check_to_var, cmap="RdBu_r", aspect="auto", vmin=-5, vmax=5)
        ax3.set_title("Check â†’ Variable Messages")
        ax3.set_xlabel("Variable Node")
        ax3.set_ylabel("Check Node")
        plt.colorbar(im3, ax=ax3, shrink=0.8)

        # Convergence tracking
        if iteration > 0:
            belief_changes = [np.linalg.norm(np.array(self.belief_history[i + 1]) - np.array(self.belief_history[i])) for i in range(iteration)]
            ax4.plot(range(1, iteration + 1), belief_changes, "o-", color=modern_palette[0], linewidth=2, markersize=8)
            ax4.set_title("Belief Convergence")
            ax4.set_xlabel("Iteration")
            ax4.set_ylabel("L2 Norm of Belief Change")
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, "Initial State\n(No convergence data)", ha="center", va="center", transform=ax4.transAxes, fontsize=14, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
            ax4.set_title("Belief Convergence")

        return fig


# %%
# Run Belief Propagation Visualization
# --------------------------------------

# Create LDPC encoder and check its dimensions
encoder = LDPCCodeEncoder(H_matrix)
print(f"Encoder code dimension: {encoder.code_dimension}")
print(f"Encoder code length: {encoder.code_length}")
print(f"Generator matrix shape: {encoder.generator_matrix.shape}")

# Generate correct message size based on encoder dimensions
message_bits = torch.randint(0, 2, (encoder.code_dimension,), dtype=torch.float32)
codeword = encoder(message_bits.unsqueeze(0)).squeeze()

print(f"\nOriginal message: {message_bits.int().tolist()}")
print(f"Encoded codeword: {codeword.int().tolist()}")

# Add noise
snr_db = 2.0
channel = AWGNChannel(snr_db=snr_db)
bipolar_codeword = 2.0 * codeword - 1.0
received = channel(bipolar_codeword.unsqueeze(0)).squeeze()

# Convert to LLRs (assuming BPSK)
noise_variance = 10 ** (-snr_db / 10)
received_llrs = 2 * received / noise_variance

print(f"Received signal: {received.numpy()}")
print(f"Received LLRs: {received_llrs.numpy()}")

# Create and run visualizer
bp_viz = BeliefPropagationVisualizer(H_matrix.numpy(), received_llrs.numpy())
bp_viz.run_iterations()

# Show key iterations
for iteration in [0, 1, 3, 5]:
    bp_viz.visualize_iteration(iteration)
    plt.show()

# %%
# Performance Comparison with Different Parameters
# ------------------------------------------------


def compare_ldpc_performance():
    """Compare LDPC performance with different decoder parameters."""

    # Test parameters
    snr_range = np.arange(-2, 8, 1)
    iteration_counts = [1, 5, 10, 20]
    num_trials = 100

    results = {}

    for max_iters in iteration_counts:
        ber_values = []

        for snr_db in tqdm(snr_range, desc=f"Testing {max_iters} iterations"):
            errors = 0
            total_bits = 0

            channel = AWGNChannel(snr_db=snr_db)
            decoder = BeliefPropagationDecoder(encoder, bp_iters=max_iters)

            for _ in range(num_trials):
                # Generate random message with correct dimension
                msg = torch.randint(0, 2, (1, encoder.code_dimension), dtype=torch.float32)
                codeword = encoder(msg)

                # Transmit through channel
                bipolar = 2.0 * codeword - 1.0
                received = channel(bipolar)

                # Decode
                decoded = decoder(received)

                # Count errors
                errors += torch.sum(msg != decoded).item()
                total_bits += msg.numel()

            ber = errors / total_bits
            ber_values.append(ber)

        results[max_iters] = ber_values

    return snr_range, results


# Run performance comparison
print("\nRunning performance comparison...")
snr_range, perf_results = compare_ldpc_performance()

# %%
# Visualize Performance Results
# --------------------------------------

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7), constrained_layout=True)

# BER vs SNR plot
for i, (max_iters, ber_values) in enumerate(perf_results.items()):
    color = modern_palette[i % len(modern_palette)]
    ax1.semilogy(snr_range, ber_values, "o-", color=color, linewidth=2.5, markersize=8, label=f"{max_iters} iterations")

ax1.grid(True, which="both", alpha=0.3)
ax1.set_xlabel("SNR (dB)", fontsize=12, fontweight="bold")
ax1.set_ylabel("Bit Error Rate", fontsize=12, fontweight="bold")
ax1.set_title("LDPC Performance vs. Decoder Iterations", fontsize=14, fontweight="bold")
ax1.legend(fontsize=11)
ax1.set_ylim(1e-4, 1)

# Convergence benefit visualization
snr_target = 4  # dB
snr_idx = np.where(snr_range == snr_target)[0][0]
iterations = list(perf_results.keys())
bers_at_target = [perf_results[it][snr_idx] for it in iterations]

bars = ax2.bar(range(len(iterations)), bers_at_target, color=modern_palette[: len(iterations)], alpha=0.8, edgecolor="black")
ax2.set_yscale("log")
ax2.set_xlabel("Number of Iterations", fontsize=12, fontweight="bold")
ax2.set_ylabel("Bit Error Rate", fontsize=12, fontweight="bold")
ax2.set_title(f"BER vs. Iterations at SNR = {snr_target} dB", fontsize=14, fontweight="bold")
ax2.set_xticks(range(len(iterations)))
ax2.set_xticklabels(iterations)
ax2.grid(True, axis="y", alpha=0.3)

# Add value labels on bars
for bar, ber in zip(bars, bers_at_target):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width() / 2.0, height * 1.1, f"{ber:.1e}", ha="center", va="bottom", fontsize=10, fontweight="bold")
plt.show()

# %%
# Code Construction Analysis
# --------------------------------------


def analyze_code_structure(H):
    """Analyze the structure of an LDPC code."""
    m, n = H.shape

    # Calculate degrees
    var_degrees = np.sum(H, axis=0)
    check_degrees = np.sum(H, axis=1)

    # Calculate girth (approximate)
    # Create adjacency matrix for the Tanner graph
    # This is a simplified girth calculation
    adj_matrix = np.zeros((m + n, m + n))
    for i in range(m):
        for j in range(n):
            if H[i, j] == 1:
                adj_matrix[i, m + j] = 1
                adj_matrix[m + j, i] = 1

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14), constrained_layout=True)
    fig.suptitle("LDPC Code Structure Analysis", fontsize=16, fontweight="bold")

    # Variable node degree distribution
    unique_var_degrees, var_counts = np.unique(var_degrees, return_counts=True)
    ax1.bar(unique_var_degrees, var_counts, color=modern_palette[0], alpha=0.8, edgecolor="black")
    ax1.set_xlabel("Variable Node Degree")
    ax1.set_ylabel("Count")
    ax1.set_title("Variable Node Degree Distribution")
    ax1.grid(True, alpha=0.3)

    # Check node degree distribution
    unique_check_degrees, check_counts = np.unique(check_degrees, return_counts=True)
    ax2.bar(unique_check_degrees, check_counts, color=modern_palette[1], alpha=0.8, edgecolor="black")
    ax2.set_xlabel("Check Node Degree")
    ax2.set_ylabel("Count")
    ax2.set_title("Check Node Degree Distribution")
    ax2.grid(True, alpha=0.3)

    # Edge distribution pattern
    ax3.imshow(H, cmap="Blues", aspect="auto", interpolation="nearest")
    ax3.set_xlabel("Variable Nodes")
    ax3.set_ylabel("Check Nodes")
    ax3.set_title("Parity Check Matrix Structure")

    # Add grid lines to show block structure
    for i in range(m + 1):
        ax3.axhline(i - 0.5, color="gray", linewidth=0.5, alpha=0.5)
    for j in range(n + 1):
        ax3.axvline(j - 0.5, color="gray", linewidth=0.5, alpha=0.5)

    # Statistics text
    stats_text = f"""Code Parameters:
    â€¢ Dimensions: ({n}, {n-m})
    â€¢ Rate: {(n-m)/n:.3f}
    â€¢ Variable node degrees: {var_degrees.tolist()}
    â€¢ Check node degrees: {check_degrees.tolist()}
    â€¢ Average variable degree: {np.mean(var_degrees):.2f}
    â€¢ Average check degree: {np.mean(check_degrees):.2f}
    â€¢ Total edges: {np.sum(H):.0f}
    â€¢ Density: {np.sum(H)/(m*n):.3f}"""

    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11, verticalalignment="top", fontfamily="monospace", bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    ax4.set_title("Code Statistics")
    ax4.axis("off")

    return fig


# Analyze the code structure
analyze_code_structure(H_matrix.numpy())
plt.show()

# %%
# Conclusion
# --------------------------------------
print("\n" + "=" * 60)
print("ADVANCED LDPC VISUALIZATION SUMMARY")
print("=" * 60)
print(
    """
This example demonstrated advanced visualization techniques for LDPC codes:

ðŸ”¹ Enhanced Tanner Graphs:
  â€¢ Node sizing based on degree
  â€¢ Connection weighting by importance
  â€¢ Degree distribution analysis

ðŸ”¹ Belief Propagation Animation:
  â€¢ Message passing visualization
  â€¢ Convergence tracking
  â€¢ LLR evolution over iterations

ðŸ”¹ Performance Analysis:
  â€¢ Iteration count comparison
  â€¢ SNR sensitivity analysis
  â€¢ Convergence behavior study

ðŸ”¹ Code Structure Analysis:
  â€¢ Degree distribution patterns
  â€¢ Sparsity characteristics
  â€¢ Statistical properties

Key Insights:
â€¢ Higher iteration counts improve performance but with diminishing returns
â€¢ Node degree affects convergence behavior
â€¢ Sparse structure enables efficient belief propagation
â€¢ Visual analysis helps in code design and optimization
"""
)
